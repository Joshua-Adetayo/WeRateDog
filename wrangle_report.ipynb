{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wrangle_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0 Introduction\n",
    "This brief report provides an overview of the data wrangling procedures used to complete the \"WeRateDogs\" project for Udacity's Data Analysis Nanodegree. The tweet history of Twitter account @dog rates, better known as WeRateDogs, makes up the dataset. WeRateDogs is a Twitter account that rates users' dogs and adds a lighthearted comment.\n",
    "The steps in the DataWrangling process are: 1. Data collection 2. assessment of the data 3. The data's cleaning 4. storing the data.\n",
    "\n",
    "#### 1.1 Data Gathering\n",
    "Three sources were used to collect the data for this project. Each one tests a unique method for gathering a dataset.\n",
    "The first dataset was manually obtained as a csv file with the name \"twitter_\n",
    "archive enhanced.csv \"to a pandas DataFrame, and was read. The most data is contained in this file.\n",
    "\n",
    "According to a neural network, a file called \"image predictions.tsv\" is contained in every tweet in the second dataset. It was downloaded programmatically using the Requests library and is stored on servers owned by Udacity.\n",
    "The third and final dataset, \"tweet json.txt,\" was collected by utilizing the tweet IDs from the WeRateDogs Twitter archive to use Python's Tweepy package to ask the Twitter API for each tweet's JSON data.\n",
    "\n",
    "#### 1.2 Assessing Data\n",
    "Following the collection of all three data sets, they were visually and programmatically evaluated for quality and tidiness issues. In order to make the data eligible for analysis, the datasets were evaluated with the goal of finding at least eight quality issues and two tidiness issues.\n",
    "\n",
    "Quality refers to issues with the data's content, also known as dirty data. To identify quality issues, the standard criteria of completeness, validity, accuracy, and consistency of the data were used. While tidiness issues are structural issues, they are sometimes referred to as messy data.\n",
    "To visually assess the dataset, some python syntax such as \"df.head()\", \"df.info()\", and so on were used, and some quality issues such as \"wrong datatypes\" were observed. As these quality issues were discovered, they were documented.\n",
    "\n",
    "The dataset was then further assessed using some functions, and some tidiness issues were also observed and documented. The assessment was completed with certain specifications in mind, as outlined in the \"Step 2: Assessing data\" section on the Udacity classroom page. According to the assessment, there were eleven (11) quality issues and three (3) tidiness issues in the \"Accessing Data\" section of the wrangle act.ipynb Jupyter Notebook.\n",
    "\n",
    "#### 1.3 Data Cleaning\n",
    "We began cleaning the data after assessing the datasets and documenting the quality and tidiness issues that needed to be addressed. The cleaning procedure followed the standard procedure of stating the problem, defining the problem, writing the code, and testing to ensure the change was effective.\n",
    "\n",
    "First, we copied the datasets and began addressing the documented issues. Some of the quality issues that were cleaned were as follows: first, we corrected the datatypes of some variables, then we used a function to extract the source of the tweets. We noticed some dog names were unrealistic while assessing, so we replaced some with their actual names and changed others to null values.\n",
    "\n",
    "The first tidiness issue we addressed was merging the three datasets into one dataframe with the reference \"tweet id.\" The merged dataframe is then saved as \"merged data,\" and the cleaning process is continued. In addition, we remove columns that had a high number of missing values and were not necessary for analysis.\n",
    "Following the resolution of the documented quality and tidiness issues, our new dataframe \"merged data\" was reduced to 12 columns and 1930 observations, making it very clean and suitable for further analysis and visualization.\n",
    "\n",
    "#### 1.4 Storing Data\n",
    "The tidy master dataset \"merged data\" was then saved as a CSV file called \"twitter_ archive master.csv.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
